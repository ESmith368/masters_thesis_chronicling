{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''imports'''\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import collections\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_soup(file_path):\n",
    "    '''function takes file path to xml document reads in a uses bs to create and return soup object of the whole file.'''\n",
    "    content = list()\n",
    "    # Read the XML file\n",
    "    with open(file_path, \"r\", encoding= 'utf-8') as file:\n",
    "        # Read each line in the file, readlines() returns a list of lines\n",
    "        content = file.readlines()\n",
    "        # Combine the lines in the list into a string\n",
    "        content = \"\".join(content)\n",
    "        bs_content = bs(content, \"lxml\")\n",
    "        \n",
    "        return bs_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexed_kopie_line_dict(bs_content):\n",
    "    '''function takes the soup object outputted from make_soup finds all l labels and uses this information to find all kopie\n",
    "    labels within them. Returns a list of dictionaries, where each dictionary has \n",
    "    three keys.\n",
    "    text: The text from the line\n",
    "    index: The index of the line within the text body\n",
    "    kopie: binary 0/1 where 1 is kopie and 0 is non-kopie tagged'''\n",
    "    \n",
    "    #finding all ines ('l' tags)\n",
    "    everything_results = bs_content.find_all(\"l\")\n",
    "    \n",
    "    #creating the list of dictionaries\n",
    "    dict_list = list()\n",
    "    n = 0\n",
    "    everything_results = bs_content.find_all(\"l\")\n",
    "    for item in list(everything_results):\n",
    "        item_string = str(item)\n",
    "        temp_dict =dict()\n",
    "        n+=1\n",
    "        if '<kopie' in item_string:\n",
    "            xml = item.find('kopie')\n",
    "            temp_dict['text'] = xml.text\n",
    "            temp_dict['index']= n\n",
    "            temp_dict['kopie']= 1\n",
    "            dict_list.append(temp_dict)\n",
    "            tokenized_item = nltk.word_tokenize(item.text)\n",
    "            tokenized_xml = nltk.word_tokenize(xml.text)\n",
    "            c = collections.Counter(tokenized_item) - collections.Counter(tokenized_xml)                   \n",
    "            text_diff = ' '.join(c.elements())\n",
    "            if len(text_diff) != 0:\n",
    "                diff_dict = dict()\n",
    "                diff_dict['text'] = text_diff\n",
    "                diff_dict['index'] = n\n",
    "                diff_dict['kopie'] = 0\n",
    "                dict_list.append(diff_dict)\n",
    "        else:\n",
    "            temp_dict['text'] =item.text\n",
    "            temp_dict['index'] = n\n",
    "            temp_dict['kopie'] = 0\n",
    "            dict_list.append(temp_dict)\n",
    "            \n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranges(nums):\n",
    "    '''finds ranges of consecutive ints in a given sequence'''\n",
    "    nums = sorted(set(nums))\n",
    "    gaps = [[s, e] for s, e in zip(nums, nums[1:]) if s+1 < e]\n",
    "    edges = iter(nums[:1] + sum(gaps, []) + nums[-1:])\n",
    "    return list(zip(edges, edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_groups(new_df):\n",
    "    '''Function takes a DataFrame representation of the output of indexed_kopie_line_dict and first seperates the kopies and\n",
    "    non-kopies before finding the consecutive index ranges for each group. Outputs two objest, each is a list of tuples which\n",
    "    represent the consecutive index ranges. the first is for kopie and the second is for non-kopie'''\n",
    "    \n",
    "    #seperating kopies and non-kopies\n",
    "    kopie_bool = new_df['kopie'] == 1\n",
    "    kopie_df = new_df[kopie_bool]\n",
    "    no_kopie_bool = new_df['kopie'] == 0\n",
    "    no_kopie_df = new_df[no_kopie_bool]\n",
    "    \n",
    "    #getting index ranges\n",
    "    index_groups_kopie = ranges(kopie_df['index'])\n",
    "    index_groups_no_kopie = ranges(no_kopie_df['index'])\n",
    "    \n",
    "    return index_groups_kopie, index_groups_no_kopie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_row_locs(index_groups_list, new_df):\n",
    "    '''function takes a list of tuples representing the index groups and finds the location of the rows for the lines within\n",
    "    the index range in the large df.\n",
    "    Returns a list of dictionaries with two keys.\n",
    "    index_range: the tuple representing the the start and end index of the chunk.\n",
    "    loc_list: the list containing the locations for each of the rows in new_df which are part of the chunk'''\n",
    "    \n",
    "    loc_main = list()\n",
    "    for group in index_groups_list:\n",
    "        loc_group_dict =dict()\n",
    "        loc_group_dict['index_range'] = group\n",
    "        loc_list = list()\n",
    "        for i in range (group[0],group[1]+1):\n",
    "            loc_list += list(new_df.loc[new_df['index'] == i].index)\n",
    "        loc_group_dict['loc_list'] = loc_list\n",
    "        loc_main.append(loc_group_dict)\n",
    "        \n",
    "    return loc_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_collected_lists(new_df, locs_list, tag = 'kopie'):\n",
    "\n",
    "    main_list = list()\n",
    "    for group_dict in locs_list:\n",
    "        temp_dict = dict()\n",
    "\n",
    "        if tag == 'kopie':\n",
    "            temp_dict['kopie'] = 1\n",
    "            kopie_df = new_df.loc[group_dict['loc_list']]\n",
    "            filtered = kopie_df.loc[kopie_df['kopie']==1]\n",
    "            group_text = list(filtered['text'])\n",
    "            temp_dict['text'] = \" \".join(group_text) \n",
    "\n",
    "        else:\n",
    "            temp_dict['kopie'] = 0\n",
    "            no_kopie_df = new_df.loc[group_dict['loc_list']]\n",
    "            filtered = no_kopie_df.loc[no_kopie_df['kopie']==0]\n",
    "            group_text= list(filtered['text'])\n",
    "            temp_dict['text'] = \" \".join(group_text) \n",
    "\n",
    "        temp_dict['index_range'] = group_dict['index_range']\n",
    "\n",
    "        main_list.append(temp_dict)\n",
    "\n",
    "    return main_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_final_df(kopie_list, no_kopie_list):\n",
    "    '''function takes the two lists of dictionaries obtained from running make_collected_lists on the kopies and non-kopies\n",
    "    and joins them together before creating and returning a large df with all the information.'''\n",
    "    \n",
    "    just_nk_df = pd.DataFrame(no_kopie_list)\n",
    "    just_k_df = pd.DataFrame(kopie_list)\n",
    "    \n",
    "    frames= [just_nk_df, just_k_df]\n",
    "    concatenated_df = pd.concat(frames)\n",
    "    \n",
    "    sorted_df = concatenated_df.sort_values(by ='index_range' )\n",
    "    \n",
    "    return sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function(filepath):\n",
    "    \n",
    "    print('reading input file...')\n",
    "    \n",
    "    bs_content = make_soup(filepath)\n",
    "    \n",
    "    dict_list = indexed_kopie_line_dict(bs_content)\n",
    "    \n",
    "    new_df = pd.DataFrame(dict_list)\n",
    "    new_df = new_df.loc[(new_df['text'] != '') & (new_df['text'] != ' ')]\n",
    "    for i, row in new_df.iterrows():\n",
    "        text = row['text']\n",
    "        no_hyphen = text.replace('Â¬', ' ')\n",
    "        no_punc = re.sub(r'[^\\w\\s]','',no_hyphen)\n",
    "        new_df.at[i,'text'] = no_punc\n",
    "        \n",
    "    print('generating fragments...')\n",
    "    \n",
    "    index_groups_kopie = get_index_groups(new_df)[0]\n",
    "    index_groups_no_kopie = get_index_groups(new_df)[1]\n",
    "    \n",
    "    locs_kopie = collect_row_locs(index_groups_kopie, new_df)\n",
    "    locs_no_kopie = collect_row_locs(index_groups_no_kopie, new_df)\n",
    "    \n",
    "    kopie_list = new_collected_lists( new_df, locs_kopie, tag= 'kopie')\n",
    "    no_kopie_list = new_collected_lists(new_df, locs_no_kopie, tag= 'no_kopie')\n",
    "    \n",
    "    data = make_final_df(kopie_list, no_kopie_list)\n",
    "    data['text'] = data['text'].str.replace('\\d+', '')\n",
    "    df_over_100 = data[data['text'].apply(lambda x: len(nltk.word_tokenize(x)) > 100)]\n",
    "    \n",
    "    print(f'writing {df_over_100.shape[0]} fragments to text files...')\n",
    "    \n",
    "    for index, row in df_over_100.iterrows():\n",
    "    \n",
    "        if row['kopie'] == 1:\n",
    "            filename = 'kopie_' + str(row['index_range'][0])+'_'+str(row['index_range'][1]) +'.txt'\n",
    "            opening = open(filename, 'w', encoding = 'utf-8')\n",
    "            opening.writelines(row['text'])\n",
    "            opening.close()\n",
    "\n",
    "        else:\n",
    "            filename = 'no_kopie_' + str(row['index_range'][0])+'_'+str(row['index_range'][1]) +'.txt'\n",
    "            opening = open(filename, 'w', encoding= 'utf-8')\n",
    "            opening.writelines(row['text'])\n",
    "            opening.close()\n",
    "        \n",
    "    print('files ready')\n",
    "    \n",
    "    return df_over_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file...\n",
      "generating fragments...\n",
      "writing 299 fragments to text files...\n",
      "files ready\n"
     ]
    }
   ],
   "source": [
    "main_function(\"C:/Users/Ellie/Documents/MASTERS/NIAA/new_allie/1791_Purm_Louw_kopie_tei.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
